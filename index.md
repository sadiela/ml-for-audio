# Website for the Machine Learning for Audio Workshop

The Machine Learning for Audio Workshop at NeurIPS 2023 will bring together audio practitioners and machine learning researchers to a venue focused on various problems in audio, including music information retrieval, acoustic event detection, computational paralinguistics, speech transcription, multimodal modeling, and generative modeling of speech and other sounds.

## Workshop Description
Audio research has recently been enjoying a renaissance of sorts; in the field of audio synthesis alone, many prominent papers have been released in just the past calendar year, with no sign of slowing down. There are numerous additional key problems within the audio research domain that continue to attract widespread attention. We believe that a workshop focused on machine learning in the audio domain would provide a good opportunity to bring together both practitioners of audio tools along with core machine learning researchers interested in audio, in order to foster collaboration and discussion as well as forge new directions within this important area of research. In addition, with the field moving so rapidly, we believe this workshop will provide a dedicated space for the crucial ethical discussions that must be facilitated among researchers around applications of generative machine learning for audio.

**The Machine Learning for Audio workshop at NeurIPS 2023 will cover a broad range of tasks involving audio data. These include, but are not limited to: methods of speech modeling, environmental sound generation or other forms of ambient sound, novel generative models, music generation in the form of raw audio, text-to-speech methods, denoising of speech and music, data augmentation, classification of acoustic events, transcription, source separation, and multimodal problems.**

We plan to solicit original workshop papers in these areas, which will be reviewed by the organizers and an additional set of reviewers. We also plan to run a demo session alongside the poster session, where contributors will be able to present live demos of their work where applicable. We believe this session will complement the newly announced Creative AI Track very nicely; as synthesis is a prominent subfield within audio machine learning research, we will be able to further highlight novel generative methods that do not necessarily overlap with a creative application.

## Data Release 
Recognizing the scarcity of free, publicly available audio data, Modulate and Hume AI will contribute several datasets in the speech domain alongside the workshop, all of large scale for their respective domains. These datasets will include acted speech (professionally acted scripts), spontaneous speech (streamer content), mimicked speech (short-form emotive recordings), and mimicked non-verbal speech. In a pseudo-competition style, each dataset will be described, and baseline results for various audio modeling tasks will be released via a white paper. Further, the white paper will also give a brief overview of other open-source audio data for researchers to explore. The organizers hope this allows researchers from smaller research groups and academia to work with and validate findings on larger, more generalizable datasets.

## Invited Speakers
* **[Shoko Araki](https://scholar.google.com/citations?user=bNo2kYsAAAAJ&hl=enJ)** received her B.E. and M.E. degrees in mathematical engineering and information physics from the University of Tokyo and her PhD in information science from Hokkaido University. In 2000, she joined NTT Communication Science Laboratories, Kyoto. She is a member of the IEEE, IEICE, and the ASJ. Her research interests include array signal processing, blind source separation applied to speech signals, speech diarization, and auditory scene analysis.
* **[Rachel Bittner](https://scholar.google.com/citations?hl=en&user=pXn1kQEAAAAJ)** is a senior research scientist at Spotify, and completed her Ph.D. at New York University in the Music and Audio Research Lab (MARL) working with Juan Bello. Previously, Rachel was a research assistant at NASA Ames Research Center working with Durand Begault in the Advanced Controls and Displays Laboratory. Rachel completed her master's in math at NYU's Courant Institute, and her bachelor's in music performance and math at UC Irvine. Her research areas of interest include automatic music transcription, source separation, musical version identification, open-source software and datasets for music research.
* **[Dimitra Emmanouilidou](https://scholar.google.com/citations?hl=en&user=S1CeYaQAAAAJ)** is a member of the Audio and Acoustics Research group within Microsoft Research Lab at Redmond. The Audio and Acoustics group conducts research in audio processing and speech enhancement, 3D audio perception and technologies, devices for audio capture and rendering, array processing, and information extraction from audio signals. Her research areas of interest include signal processing, audio understanding and audio analytics.
* **[Ben Hayes](https://scholar.google.com/citations?hl=en&user=Zq2uWSkAAAAJ)** is a PhD student in Artificial Intelligence and Music at Queen Mary University of London’s Centre for Digital Music. He is jointly supervised by Dr Charalampos Saitis and Dr György Fazekas. Previously, he was Music Lead at the award-winning AI-driven generative music startup Jukedeck, and was a research intern with ByteDance’s Speech, Audio \& Music Intelligence (SAMI) team. He also makes music and teaches undergraduate Electronic and Produced music at the Guildhall School of Music and Drama. His research interests include audio synthesis, differentiable signal processing, meta-learning, and musical timbre.
* **[Bjorn Schuller](https://scholar.google.com/citations?user=TxKNCSoAAAAJ&hl=en)** is a Full Professor \& Head of the Chair of Embedded Intelligence for Health Care and Wellbeing at University of Augsburg, Germany. He is also a Professor of Artificial Intelligence \& Head of the Group on Language, Audio \& Music at Imperial College London, Chief Scientific Officer (CSO) and Co-Founding CEO at audEERING GmbH, and a Visiting Professor at the School of Computer Science and Technology, Harbin Institute of Technology in Harbin/P.R. China. His research areas of interest include computer audition for health and computational paralinguistics.
* **[Neil Zeghidour](https://scholar.google.com/citations?user=fiJamZ0AAAAJ&hl=fr)** is a research scientist at Google Brain. He generates audio with machines, with recent work including SoundStream, AudioLM, and MusicLM. His research areas of interest include audio classification and synthesis.

## Organizers
* **[Sadie Allen](https://scholar.google.com/citations?user=LrmTlQwAAAAJ&hl=en&oi=ao)** *(she/her)* is a PhD student studying computer engineering at Boston University. She co-organized the Machine Learning for Audio Synthesis workshop at ICML 2022. Her current research focuses on controllable music generation in both the symbolic and raw audio domains. Her previous work centered around the security and efficiency of distributed systems. 
* **[Alice Baird](https://scholar.google.com/citations?user=fHQwc60AAAAJ&hl=en&oi=ao)** *(she/her)* is a research scientist at Hume AI, NY, USA, where she currently works on modeling expressive human behaviors from audio and other modalities.  She earned her Ph.D. at the University of Augsburg, in 2022. Her work on emotion understanding from auditory, physiological, and multimodal data has been published extensively in the leading journals and conferences in her field. She has co-organized, several machine learning competitions including the 2022 ICML Expressive Vocalizations Workshop.
* **[Alan Cowen](https://scholar.google.com.sg/citations?user=-i9gbsAAAAAJ&hl=en)** *(he/him)* is an applied mathematician and computational emotion scientist developing new data-driven methods to study human experience and expression. He was previously a researcher at the University of California and visiting scientist at Google, where he helped establish affective computing research efforts. His discoveries have been featured in top journals such as Nature, PNAS, Science Advances, and Nature Human Behavior (i10-index: 16) and covered in press outlets ranging from CNN to Scientific American. His research applies new computational tools to address how emotional behaviors can be evoked, conceptualized, predicted, and annotated, how they influence our social interactions, and how they bring meaning to our everyday lives.
* **[Sander Dieleman](https://scholar.google.com/citations?hl=en&user=yNNIKJsAAAAJ)** *(he/him)* is a research scientist at DeepMind in London, UK, where he has worked on the development of AlphaGo and WaveNet. His research is currently focused on generative modelling of perceptual signals at scale, including audio (speech \& music) and visual data. He has previously co-organised four editions of the NeurIPS workshop on machine learning for creativity and design (2017-2020) and three editions of the Recsys workshop on deep learning for recommender systems (DLRS 2016-2018). He also co-organized the Machine Learning for Audio Synthesis workshop at ICML last year.
* **[Brian Kulis](https://scholar.google.com/citations?hl=en&user=okcbLqoAAAAJ)** *(he/him)* is an associate professor at Boston University and former Amazon scholar who worked on Alexa. His research focuses broadly on machine learning, recently focused on applications to audio problems such as detection and generation.  He has won two best paper awards at ICML and a best paper award at CVPR. He has previously organized two workshops at ICCV (in 2011 and 2013), one workshop at NeurIPS (in 2011), and two workshops at ICML (in 2019 and 2022).  He is regularly an area or senior area chair at major AI conferences, was the local arrangements chair for CVPR in 2014, and has organized tutorials at ICML and ECCV.
* **[Rachel Manzelli](https://scholar.google.com/citations?hl=en&user=BzMNxvoAAAAJ)** *(she/her)* is a senior machine learning engineer at Modulate, where she works on both audio generation and classification models to assist video game moderation teams in decreasing toxicity in voice chat. She co-organized the Machine Learning for Audio Synthesis workshop at ICML 2022. She earned her bachelor's degree in computer engineering from Boston University in 2019. During her undergraduate career, she conducted research in the areas of structured music generation and MIR.
* **[Shrikanth Narayanan](https://scholar.google.com/citations?hl=en&user=8EDHmYkAAAAJ)** *(he/him)* is a University Professor and holder of the Niki and Max Nikias Chair in Engineering at the University of Southern California (USC). Shri is a Fellow of the National Academy of Inventors (NAI), the Acoustical Society of America (ASA), the Institute of Electrical and Electronics Engineers (IEEE), the International Speech Communication Association (ISCA), the Association for Psychological Science (APS), the American Association for the Advancement of Science (AAAS), American Institute for Medical and Biological Engineering (AIMBE) and the Association for the Advancement of Affective Computing (AAAC). Shri Narayanan is a member of the European Academy of Sciences and Arts and a 2022 Guggenheim Fellow.
